---
layout: tool
title: velma
description: "Value extraction via language model abduction."
image: /assets/img/velma_featured.png
published: True
---

![](/assets/img/velma_featured.png)

**velma**: value extraction via language model abduction **([repo](https://github.com/paulbricman/velma), [demo](https://huggingface.co/spaces/paulbricman/velma))**

| tl;dr                                                                                                                                                                                                             |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| We attempt to automatically infer one's beliefs from their writing in three different ways. Initial results based on Twitter data hint at embeddings and language models being particularly promising approaches. |

In this project, [Youssef](https://youssefabdelm.github.io/) and I had a look at different means of recovering beliefs from someone's writing. We identify beliefs with statements about the world (e.g. "White chocolate is best chocolate.") paired with a measure of credence (i.e. how strong the belief is held). Additionally, we take belief systems to refer to a large set of such beliefs which are simultaneously held by an individual.

Being able to automatically infer one's credence towards a statement based on their writing is useful in many ways:

1. **rapid sensemaking**: Given a body of trusted sources (e.g. individual experts, media outlets, scientific literature), evaluating a new claim against their belief systems might help make [quick educated guesses](/reflections/liquid-epistemics) in complex and rapidly changing situations. While individual usage is appealing in itself, [institutions might particularly benefit](https://80000hours.org/problem-profiles/improving-institutional-decision-making/) from being able to quickly get the "pulse" of a relevant body of experts.
2. **surfacing evidence**: Instead of only making use of an average credence estimate, it might also be handy to quickly surface strong evidence for and against a given claim pooled from trusted sources. By having units of content (e.g. paragraphs, documents) individually "vote" for the claim in question, it then becomes possible to simply surface the extremes, perhaps after clustering to reduce redundancy.
3. **forecasting**: Obtaining credence estimates from diverse and preferably independent sources might be convenient in refining [forecasting models](https://www.metaculus.com/questions/). The spread of "votes" by different sources for a given claim enables the use of confidence intervals. Weighting votes by recency, source track record, and open science practices might also be helpful.
4. **epistemic spectra**: While obtaining credence estimates for a given claim from different people might be useful for updating one's beliefs, it might also be worthwhile to place the separate sources themselves on a range between "totally disagree" and "totally agree." This might help paint a picture of how different public figures are deemed to relate to various claims, potentially democratizing political intelligence for activists who otherwise lack resources, or helping promote diverse views in consequential discussions.
5. **language model alignment**: Beliefs often approach the quality of values (e.g. "It's important that we cater to the wellbeing of future generations."). If we're able to estimate the alignment between arbitrary text and certain such beliefs, we should be able to _reward_ language models when we recognize desirable behavior (e.g. via [PPO](https://github.com/lvwerra/trl), obviating the need for [curated datasets](https://openai.com/blog/improving-language-model-behavior/) or [crowdsourced preferences](https://openai.com/blog/fine-tuning-gpt-2/)).
6. **truthful language models**: Considering a hypothetical knowledge base of accurate beliefs, we might be able to condition language models (e.g. via [PPLM](https://github.com/uber-research/PPLM)) to speak in such a way so as to exhibit high credence for established facts. As opposed to the previous use case, this knowledge base might be tweaked more frequently, hence a focus on online [ways of steering language models](/reflections/wielding-language-models), as opposed to ones which involve training.

The breadth of downstream applications encouraged us to look into how different technical approaches fare in terms of accurately inferring beliefs from text. A very related task, however, is stance detection. Unfortunately, most datasets focused on this task -- and hence most models fine-tuned on the them -- only address a finite set of topics (e.g. climate change, abortion), rather than an open-ended set of arbitrary statements. We were precisely interested in being able to infer a stance with regards to a custom statement.

Another related task is natural language inference (NLI), and while it might seem exactly what we're looking for (i.e. "Determine whether this writing entails or contradicts this claim."), it traditionally focuses on _deductive_ inference. We reasoned that beliefs are [represented in writing in extremely subtle and implicit ways](https://markusstrasser.org/extracting-knowledge-from-literature/), prompting a need to read between the lines in indirect ways that conventional NLI training data doesn't demand. We therefore moved towards _abductive_ inference, the approach of picking the outcome which is most likely to follow from the premise. Concretely, we compare a claim and its negation.

When thinking about how to evaluate different approaches, we initially considered using a survey to pair the user's online writing (e.g. blog posts, comments, tweets) with credences they specify on a Likert scale for a diverse set of predefined claims. Then we'd have tried to predict the credence for each claim given their writing. However, we first wanted some initial proof-of-concept results, so instead of jumping straight into running a survey, we first used what people _already stated_ online as claims they have high credence in, and left the Likert scales for future work.

Concretely, we picked 10 Twitter accounts and fetched their most recent tweets via the Twitter API. We mostly opted for people related to AI alignment (e.g. Richard Ngo), rationality (e.g. Scott Alexander), and effective altruism (e.g. William MacAskill), in an attempt to make the results easy to understand for what we thought was the likely audience of this project. We then manually extracted ~200 bold claims from their writing, in what might have been the most productive two hours of scrolling through tweets. We then manually negated those claims with as few edits as possible. The task is then to determine which of the two claims was the original one (i.e. P or Â¬P), based on the user's other tweets. As this requires an understanding of the user's view on the topic as an instrumental goal, we chose this as the means of operationalizing the ability to infer beliefs from text.

![](/assets/img/velma_approaches.svg)
_Overview of the three approaches we used for recovering the original claim based on the user's other tweets (see below)._

We used two baselines to provide a frame of reference in terms of performance. First, we check which of the two candidate claims is closer in meaning to the user's other tweets by means of cosine similarity with embeddings. Second, we use pretrained NLI models and check which of the two candidate claims is most likely to be entailed by the user's other tweets. Our main approach, termed velma, relies on pretrained language models. It consists of checking which of the two candidate claims is most likely to follow from the context as pure text, by using the user's other tweets as a prompt.

| method               | accuracy |
| -------------------- | -------- |
| embeddings           | 72%      |
| NLI                  | 62%      |
| velma (DistilGPT2)   | 55%      |
| velma (GPT-Neo 1.3B) | 67%      |

When it comes to results, both baselines and our main approach fare better than chance. The embedding baseline is surprisingly competitive, outperforming every other approach considered, which is okay. However, our main approach, the one based on pretrained language models, appears to scale well with model size. The larger of the two such models we tried (i.e. [GPT-Neo 1.3B](https://www.eleuther.ai/projects/gpt-neo/)), drastically improves in performance over the smaller one (i.e. [DistilGPT2](https://huggingface.co/distilgpt2)), which is barely above chance. This hints at the possibility that the more intricate models of the world internalized by larger language models enable more sensible predictions of likelihood -- better reading between the lines. When [scaling our infrastructure](/lifelong-learning) beyond a potato laptop, we plan on looking into whether this trend holds up.

![](/assets/img/probs.png)
_Probabilities assigned by different methods to the original claims. Estimates above 50% are deemed correct classifications._

A convenient benefit of all three approaches considered is they are highly parallelizable. Each "vote" of an individual piece of content written by a user can be computed independently, on a different machine, before being pooled together at rendezvous. Additionally, the varied performance of language models of different sizes might mean that when it's critical to gain credence estimates _quickly_, shallow models or embeddings can be employed, while if the priority is on accuracy, the deepest models can rather be put forward.

We also had a look at the relationship between semantic similarity of original claims to the other user's tweets and performance in recovering the original version from the associated pairs. The reasoning behind this was that claims which exhibited broader "coverage" in the user's writing might be easier to figure out, compared to obscure thoughts which are disconnected from the user's recurring themes. However, we only noticed a weak correlation between coverage and performance, hinting at the fact that even seemingly unlikely divergences are counterintuitively typical of the selected users.

![](/assets/img/similarity_correlations.png)
_Correlations between semantic similarity of original claims to the user's other tweets (X axis) and probability assigned for the original claims (Y axis)._

While there's still a lot to unpack, recent ML models appear quite capable of inferring beliefs from one's writing. We expect such paradigms to eventually become building blocks of [epistemic infrastructure](/reflections/liquid-epistemics), enabling both people and AI to gradually refine their beliefs at scale. Next month, it'll probably be an exploration of velma + PPO (i.e. use case #5 in the opening), mostly because of the new technical paradigm to learn about.

We stand on the shoulders of giants, who in turn stand on the shoulders of other giants, to the point where attempting to attribute precise acknowledgements for this project becomes unwieldy. Still, we thank [EleutherAI](https://www.eleuther.ai/) and [Hugging Face](https://huggingface.co/) for publishing the models we used, [Patrick Mineault](https://goodresearch.dev/) for indirectly helping us turn our poor software engineering practices into less poor ones, [Maier Fenster](https://twitter.com/maierfenster) for thoughts on abduction and the idea of use case #2, and [Aligned AI](https://buildaligned.ai/) for signaling the connection between inferring values and alignment. Paul would like to thank his [GitHub Sponsors](https://github.com/sponsors/paulbricman), especially [Andreas Stuhlmueller](https://stuhlmueller.org/).
